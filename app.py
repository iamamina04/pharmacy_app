import streamlit as st
import pandas as pd
import re
import ast
import nltk
from nltk.stem.snowball import SnowballStemmer
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from openai import OpenAI
import os

st.set_page_config(layout="wide")

# –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤
nltk.download("punkt")
nltk.download("stopwords")

# --- –°–ª–æ–≤–∞—Ä—å: –Ω–æ–º–µ—Ä –∞–ø—Ç–µ–∫–∏ ‚Üí —Ä–∞–π–æ–Ω ---
pharmacy_districts = {
    '34': '–ñ–µ—Ç—ã—Å—É—Å–∫–∏–π', '40': '–ë–æ—Å—Ç–∞–Ω–¥—ã–∫—Å–∫–∏–π', '57': '–ñ–µ—Ç—ã—Å—É—Å–∫–∏–π', '58': '–ê–ª–∞—Ç–∞—É—Å–∫–∏–π',
    '59': '–ë–æ—Å—Ç–∞–Ω–¥—ã–∫—Å–∫–∏–π', '74': '–ú–µ–¥–µ—É—Å–∫–∏–π', '75': '–ê–ª–º–∞–ª–∏–Ω—Å–∫–∏–π', '81': '–ê—É—ç–∑–æ–≤—Å–∫–∏–π',
    '83': '–ê–ª–º–∞–ª–∏–Ω—Å–∫–∏–π', '91': '–ú–µ–¥–µ—É—Å–∫–∏–π', '94': '–ê–ª–º–∞–ª–∏–Ω—Å–∫–∏–π', '132': '–¢—É—Ä–∫—Å–∏–±—Å–∫–∏–π',
    '175': '–ñ–µ—Ç—ã—Å—É—Å–∫–∏–π', '222': '–ê–ª–º–∞–ª–∏–Ω—Å–∫–∏–π', '234': '–ñ–µ—Ç—ã—Å—É—Å–∫–∏–π', '260': '–ê—É—ç–∑–æ–≤—Å–∫–∏–π',
    '263': '–ê—É—ç–∑–æ–≤—Å–∫–∏–π', '294': '–ú–µ–¥–µ—É—Å–∫–∏–π', '309': '–ë–æ—Å—Ç–∞–Ω–¥—ã–∫—Å–∫–∏–π', '311': '–ê–ª–º–∞–ª–∏–Ω—Å–∫–∏–π',
    '320': '–¢—É—Ä–∫—Å–∏–±—Å–∫–∏–π', '323': '–ê—É—ç–∑–æ–≤—Å–∫–∏–π'
}

def get_districts(pharmacies_str):
    try:
        pharmacies = ast.literal_eval(pharmacies_str)
        districts = set()
        for p in pharmacies:
            numbers = re.findall(r"\u2116\s*(\d+)", p)
            for number in numbers:
                if number in pharmacy_districts:
                    districts.add(pharmacy_districts[number])
        return list(districts)
    except:
        return []

# --- –°—Ç–µ–º–º–∏–Ω–≥ ---
stemmer = SnowballStemmer("russian")
russian_stopwords = stopwords.words("russian")

def stem_text(text):
    if not isinstance(text, str):
        return ""
    words = nltk.word_tokenize(text.lower())
    stemmed_words = [stemmer.stem(word) for word in words if word.isalpha()]
    return " ".join(stemmed_words)

@st.cache_data
def load_data():
    df = pd.read_csv("processed_products_data.csv")
    df["–ö–∞—Ç–µ–≥–æ—Ä–∏–∏_—Å—Ç—Ä–æ–∫–∞"] = df["–ö–∞—Ç–µ–≥–æ—Ä–∏–∏"].apply(
        lambda x: " ".join(eval(x)) if isinstance(x, str) and x.startswith("[") else ""
    )
    df["text"] = (
        df["–ù–∞–∑–≤–∞–Ω–∏–µ"].fillna("") + " " +
        df["–û–ø–∏—Å–∞–Ω–∏–µ"].fillna("") + " " +
        df["–ö–∞—Ç–µ–≥–æ—Ä–∏–∏_—Å—Ç—Ä–æ–∫–∞"].fillna("")
    )
    df["text_stemmed"] = df["text"].apply(stem_text)
    df["–†–∞–π–æ–Ω—ã"] = df["–ê–ø—Ç–µ–∫–∏"].apply(get_districts)
    return df

data = load_data()

@st.cache_resource
def fit_tfidf(texts):
    tfidf = TfidfVectorizer(stop_words=russian_stopwords, max_features=5000)
    matrix = tfidf.fit_transform(texts)
    return tfidf, matrix

tfidf_stemmed, tfidf_matrix_stemmed = fit_tfidf(data["text_stemmed"])

def search_products(query, top_n=5, selected_district=None):
    query_stemmed = stem_text(query)
    query_vec = tfidf_stemmed.transform([query_stemmed])
    similarity = cosine_similarity(query_vec, tfidf_matrix_stemmed).flatten()
    top_indices = similarity.argsort()[-top_n:][::-1]
    results = data.iloc[top_indices].copy()
    results["Score"] = similarity[top_indices]

    if selected_district:
        results = results[results["–†–∞–π–æ–Ω—ã"].apply(lambda r: selected_district in r if r else False)]

    return results[["–ù–∞–∑–≤–∞–Ω–∏–µ", "–û–ø–∏—Å–∞–Ω–∏–µ", "–¶–µ–Ω–∞", "–ê–ø—Ç–µ–∫–∏", "–†–∞–π–æ–Ω—ã", "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"]]

# --- AI —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è ---
API_KEY = os.getenv("TOKEN")
client = OpenAI(api_key=API_KEY, base_url="https://openrouter.ai/api/v1")

def get_product_recommendations(product_name):
    try:
        completion = client.chat.completions.create(
            model="deepseek/deepseek-chat-v3-0324:free",
            messages=[{
                "role": "user",
                "content": f"–û–ø–∏—à–∏ –∫—Ä–∞—Ç–∫–æ —Ç–æ–≤–∞—Ä {product_name} –∏ –¥–∞–π –æ—Å–Ω–æ–≤–Ω—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –≤ –æ–¥–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º."
            }]
        )
        if completion.choices:
            return {"description": completion.choices[0].message.content.strip()}
        else:
            return {"error": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –¥–ª—è —ç—Ç–æ–≥–æ —Ç–æ–≤–∞—Ä–∞"}
    except Exception as e:
        return {"error": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ API: {str(e)}"}

# --- –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å ---
st.title("üíä –ü–æ–∏—Å–∫ –ª–µ–∫–∞—Ä—Å—Ç–≤ –≤ –∞–ø—Ç–µ–∫–∞—Ö –ê–ª–º–∞—Ç—ã")

query = st.text_input("–í–≤–µ–¥–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –ª–µ–∫–∞—Ä—Å—Ç–≤–∞ –∏–ª–∏ —Å–∏–º–ø—Ç–æ–º—ã:")
districts = sorted(set(sum(data["–†–∞–π–æ–Ω—ã"], [])))
selected_district = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–∞–π–æ–Ω:", [""] + districts)

if query:
    results = search_products(query, top_n=10, selected_district=selected_district)

    if results.empty:
        st.warning("–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Ä–∞–π–æ–Ω–∞.")
    else:
        for _, row in results.iterrows():
            with st.container():
                col1, col2 = st.columns([1, 6])
                with col1:
                    if "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ" in row and isinstance(row["–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"], str):
                        st.image(row["–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"], width=200)
                with col2:
                    st.subheader(row["–ù–∞–∑–≤–∞–Ω–∏–µ"])
                    st.write(f"–¶–µ–Ω–∞: {row['–¶–µ–Ω–∞']} —Ç–≥")

                    with st.expander("–û–ø–∏—Å–∞–Ω–∏–µ"):
                        st.write(row["–û–ø–∏—Å–∞–Ω–∏–µ"])

                    if selected_district:
                        pharmacies = ast.literal_eval(row["–ê–ø—Ç–µ–∫–∏"])
                        filtered = []
                        for ph in pharmacies:
                            nums = re.findall(r"\u2116\s*(\d+)", ph)
                            for n in nums:
                                if pharmacy_districts.get(n) == selected_district:
                                    url = f"https://europharma.kz/map#{n}"
                                    filtered.append(f"[–ê–ø—Ç–µ–∫–∞ ‚Ññ{n}]({url})")
                        if filtered:
                            st.markdown(f"–ê–ø—Ç–µ–∫–∏ –≤ {selected_district}: " + ", ".join(filtered))

                    if st.button(f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –æ—Ç –ò–ò: {row['–ù–∞–∑–≤–∞–Ω–∏–µ']}", key=row['–ù–∞–∑–≤–∞–Ω–∏–µ']):
                        recommendation = get_product_recommendations(row['–ù–∞–∑–≤–∞–Ω–∏–µ'])
                        st.info(recommendation.get("description", "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"))